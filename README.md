Introduction
In this assignment, by constructing and evaluating character-based language models for Chinese, a non-Latin script language, and English, I aim to assess their structural and linguistic characteristics, as well as the challenges that arise in cross-linguistic adaptability.

Material and Methods
For the Chinese model, the dataset consisted of 1025 Pokémon names extracted from a website through web scraping. Given that Chinese names typically contain two to four characters, I applied data augmentation techniques such as character substitution, reordering, and truncation to increase variability and reduce overfitting, as the dataset was relatively small. Unlike English, where names can be split into phonemic components, Chinese follows a logographic writing system in which each character carries semantic meaning. This characteristic influenced the preprocessing approach, ensuring that the generated sequences preserved structural integrity while acknowledging occasional semantic inconsistencies.
The Chinese model was built using an LSTM-based neural network trained for 300 epochs. Special tokens (<s> for start, </s> for end, and <pad> for padding) were introduced to define sequence boundaries and ensure consistency in generation.
In contrast, the English model required a different approach due to the phonemic nature of English words. The preprocessing involved mapping characters to numerical representations using one-hot encoding, with character-to-index and index-to-character mappings facilitating the learning process. The model was trained for 10,000 epochs, significantly more than the Chinese model, due to the greater phonotactic complexity and longer word structures in English Pokémon names. To stabilize convergence, gradient clipping was employed to prevent exploding gradients, while label smoothing was applied to refine learning dynamics.
To explore cross-linguistic adaptability, an additional experiment trained the Chinese character-level model using an English dataset of Pokémon names. This aimed to determine whether a model originally designed for a logographic language could successfully learn phonemic sequences. he training process spanned 500 epochs, with loss and accuracy metrics tracked throughout.

Results
The performance of the Chinese model was assessed using perplexity and accuracy metrics. They are useful for evaluating the Chinese model because of the unique nature of Chinese as a logographic language. Unlike phonemic languages, Chinese characters represent meaning rather than phonetic units. Perplexity reflects how uncertain the model is in predicting the next character, which is important since Chinese character sequences rely on semantic and morphological patterns. Accuracy measures how well the model captures valid character combinations, which is essential in a language where incorrect character order can lead to meaningless outputs.
The model achieved a perplexity score of 110.98, which, while relatively high, was expected given the small dataset and the inherent unpredictability of creative name generation. The accuracy was measured at 29.60%, reflecting the challenge of predicting meaningful sequences in a logographic script. The generated names demonstrated structural plausibility but occasionally lacked semantic coherence. Examples include: 铁骑龙 (Tiě qí lóng) – "Iron Rider Dragon", 咕道犬 (Gū dào quǎn) – "Murmur Way Dog", 雪拉斯凯 (Xuě lā sī kǎi) – "Snow Las Kai", 露牙瓦 (Lù yá wǎ) – "Exposed Fang Tile", 电怪马 – "Electricity Monster Horse", 冰子熊 (Bīng zǐ xióng) – "Ice Cub Bear", 火鸟灵 (Huǒ niǎo líng) – "Fire Bird Spirit", 炭宝蚔 (Tàn bǎo wú) – "Charcoal Treasure Centipede". As expected, the model generated structurally sound names but lacked explicit semantic constraints, leading to occasional incoherent outputs.
One of the key challenges in working with Chinese characters is that character sequences do not neatly map to phonetic units like in English. This makes purely character-based models prone to generating syntactically correct but semantically incoherent names. Despite these limitations, the model successfully captured the structural and stylistic characteristics of Pokémon names in Chinese, generating novel yet believable outputs.
For the English model, evaluation included Levenshtein Distance, Jaccard Similarity, and a Phonetic Balance Score. The average Levenshtein Distance was 4.0400, indicating a balance between novelty and resemblance to existing names. The Jaccard Similarity score of 0.5294 suggested that the generated names shared many characters with real Pokémon names while still allowing for creative variation. The Phonetic Balance Score was 0.7301, reflecting a relatively smooth alternation between consonants and vowels, which contributed to the naturalness of the generated words. The model also compared the length distribution of generated names against real Pokémon names to ensure realism. The generated names, such as “Flazaur,” “Drakite,” and “Blitzone,” followed recognizable phonetic patterns but occasionally displayed overfitting, particularly in the repeated use of common Pokémon suffixes.
When training the Chinese model on English Pokémon names, the results were surprising. The model achieved a perplexity of 7.90, significantly lower than when trained on Chinese names, and an accuracy of 38.20%. The generated names included valid constructs such as “Tortin” and “Magnamar”, but also displayed occasional capitalization errors and hybrid formations like “Ferrosee” (a possible blend of Ferrothorn and Seedot). The improved numerical performance suggests that English names have more predictable character sequences than Chinese, making them easier to learn at a character level.
The attempt to train the English model on Chinese names failed because it could not recognize Chinese characters. Hypothetically, we could convert the characters into pinyin (the Romanized representation of Chinese characters) and then convert them back into characters. However, this approach presents another challenge: a single pinyin syllable can correspond to multiple different Chinese characters, leading to ambiguity in the reconstruction process.

Conclusion. 
While the Chinese model successfully captured structural patterns, it struggled with semantic coherence due to the absence of explicit constraints. The English model performed well but exhibited overfitting to common Pokémon name structures. The cross-linguistic experiment revealed that English names are easier to model due to their predictable character sequences, whereas Chinese presents additional challenges due to its logographic nature. Future improvements could involve integrating phonetic embeddings, subword-based models, or additional semantic constraints to enhance generation quality and adaptability across languages.
